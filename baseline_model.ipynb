{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S-GOR-YbOoz"
      },
      "source": [
        "# Baseline Modelling with ANN\r\n",
        "\r\n",
        "By: Yolanda Chen\r\n",
        "\r\n",
        "Date: February 23th, 2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L2E0LdQpDak"
      },
      "source": [
        "Resources:\r\n",
        "\r\n",
        "I think we said not to use this, but keeping it here in case we want to quickly train a model: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier.fit\r\n",
        "\r\n",
        "Again, similar process but we're going to build an ANN similar to what we've built in labs instead: https://medium.com/@sdoshi579/classification-of-music-into-different-genres-using-keras-82ab5339efe0\r\n",
        "\r\n",
        "https://medium.com/@pk_500/music-genre-classification-using-feed-forward-neural-network-using-pytorch-fdb9a960a964"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7pB2bWHnQV7"
      },
      "source": [
        "**1. Load MFCC data and labels (genre) from JSON**\r\n",
        "\r\n",
        "https://www.programiz.com/python-programming/json\r\n",
        "\r\n",
        "https://www.programiz.com/python-programming/dictionary\r\n",
        "\r\n",
        "https://stackoverflow.com/questions/55109684/how-to-handle-large-json-file-in-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZZ2dSL2TwHT"
      },
      "source": [
        "def get_data_loader(dataset, batch_size):\r\n",
        "\r\n",
        "  \"\"\" 1. Split dataset into training and testing dataset \"\"\"\r\n",
        "\r\n",
        "  # Compute the suitable lengths of the training and testing datasets\r\n",
        "  \r\n",
        "  num_train_data = int(0.8 * len(dataset))\r\n",
        "  num_test_data = int(len(dataset) - num_train_data)\r\n",
        "\r\n",
        "  # Split dataset into training and testing datasets using random_split\r\n",
        "  train_dataset, test_dataset = random_split(dataset, (num_train_data, num_test_data))\r\n",
        "\r\n",
        "\r\n",
        "  \"\"\" 2. Split original training set into a new training set and test set \"\"\"\r\n",
        "\r\n",
        "  # Randomize list of data indices\r\n",
        "  train_size = len(train_dataset)\r\n",
        "  dataset_indices = list(range(train_size))\r\n",
        "  np.random.seed(1000)\r\n",
        "  np.random.shuffle(dataset_indices)\r\n",
        "\r\n",
        "  # Split into training and testing indices\r\n",
        "  split = int(0.8 * train_size)\r\n",
        "  train_indices, test_indices = dataset_indices[:split], dataset_indices[split:]\r\n",
        "\r\n",
        "  # Split dataset into training and testing datasets using SubsetRandomSampler\r\n",
        "  train_sampler = SubsetRandomSampler(train_indices)\r\n",
        "  test_sampler = SubsetRandomSampler(test_indices)\r\n",
        "\r\n",
        "  \"\"\" 3. DataLoad the training, validation, and testing sets \"\"\"\r\n",
        "  train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, drop_last=True, sampler=train_sampler)\r\n",
        "  val_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, drop_last=True, sampler=test_sampler)\r\n",
        "  test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, drop_last=True)\r\n",
        "\r\n",
        "  return train_loader, val_loader, test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQW7O_3NHMuU"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjHt-IxmLrmM"
      },
      "source": [
        "import json\r\n",
        "\r\n",
        "# Load json file\r\n",
        "with open('/content/drive/My Drive/APS360 Project/our_spec/our_spec_new.json') as file:\r\n",
        "  data = json.load(file)\r\n",
        "\r\n",
        "# Extract mfcc and labels\r\n",
        "mfcc = data['mfcc']\r\n",
        "labels = data['labels']\r\n",
        "\r\n",
        "# Convert list to tensor and create dataset of mfcc and labels\r\n",
        "dataset = TensorDataset(torch.stack(mfcc), torch.stack(labels))\r\n",
        "train_loader, val_loader, test_loader = get_data_loader(dataset, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZF92JeL50US"
      },
      "source": [
        "Check input dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFeEk7O0rimE"
      },
      "source": [
        "**2. Multiclass-ANN Architecture**\r\n",
        "\r\n",
        "Similar to what has been done in labs\r\n",
        "\r\n",
        "Need to modify input size to match the input dimensions\r\n",
        "\r\n",
        "Start off with a small network just to check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt7x_847tO5c"
      },
      "source": [
        "class GenreClassifier(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(GenreClassifier, self).__init__()\r\n",
        "        self.layer1 = nn.Linear(700, 50)\r\n",
        "        self.layer2 = nn.Linear(50, 20)\r\n",
        "        self.layer3 = nn.Linear(20, 8) # 8 genres\r\n",
        "    def forward(self, input):\r\n",
        "        flattened = input.view(-1, 700)\r\n",
        "        activation1 = F.relu(self.layer1(flattened))\r\n",
        "        activation2 = F.relu(self.layer2(activation1))\r\n",
        "        output = self.layer3(activation2)\r\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwB1RXDB3l8y"
      },
      "source": [
        "def get_accuracy(model, train=False):\r\n",
        "\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "\r\n",
        "    if train = True\r\n",
        "      data = train_loader\r\n",
        "    else\r\n",
        "      data = val_loader\r\n",
        "\r\n",
        "    for input, labels in data:\r\n",
        "        output = model(input)\r\n",
        "\r\n",
        "        #select index with maximum prediction score\r\n",
        "        pred = output.max(1, keepdim=True)[1]\r\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\r\n",
        "        total += input.shape[0]\r\n",
        "        \r\n",
        "    return correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Yk2wuIv3nGu"
      },
      "source": [
        "def train(model, train_loader, batch_size=64, num_epochs=1):\r\n",
        "\r\n",
        "    criterion = nn.CrossEntropyLoss()\r\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\r\n",
        "\r\n",
        "    iters, losses, train_acc, val_acc = [], [], [], []\r\n",
        "\r\n",
        "    # training\r\n",
        "    n = 0 # the number of iterations\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        for input, labels in iter(train_loader):\r\n",
        "            out = model(input)            # forward pass\r\n",
        "            loss = criterion(out, labels) # compute the total loss\r\n",
        "            loss.backward()               # backward pass (compute parameter updates)\r\n",
        "            optimizer.step()              # make the updates for each parameter\r\n",
        "            optimizer.zero_grad()         # a clean up step for PyTorch\r\n",
        "\r\n",
        "            # save the current training information\r\n",
        "            iters.append(n)\r\n",
        "            losses.append(float(loss)/batch_size)             # compute *average* loss\r\n",
        "            train_acc.append(get_accuracy(model, train=True)) # compute training accuracy \r\n",
        "            val_acc.append(get_accuracy(model, train=False))  # compute validation accuracy\r\n",
        "            n += 1\r\n",
        "\r\n",
        "    # plotting\r\n",
        "    plt.title(\"Training Curve\")\r\n",
        "    plt.plot(iters, losses, label=\"Train\")\r\n",
        "    plt.xlabel(\"Iterations\")\r\n",
        "    plt.ylabel(\"Loss\")\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "    plt.title(\"Training Curve\")\r\n",
        "    plt.plot(iters, train_acc, label=\"Train\")\r\n",
        "    plt.plot(iters, val_acc, label=\"Validation\")\r\n",
        "    plt.xlabel(\"Iterations\")\r\n",
        "    plt.ylabel(\"Training Accuracy\")\r\n",
        "    plt.legend(loc='best')\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "    print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\r\n",
        "    print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeyZkdqyb9op"
      },
      "source": [
        "**3. Sanity check with small dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-VPrUutcFz6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bnHMfZGZwYi"
      },
      "source": [
        "**4. Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXi_bmY_Z3ja"
      },
      "source": [
        "model = GenreClassifier()\r\n",
        "train(model, train_loader, num_epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}